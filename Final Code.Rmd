---
title: "Time Series and Forecasting Final Project"
author: "Connor O'Brien, Ross Cole, Zakir Bekenov"
date: "6/4/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}

```

## Introduction 

In this project, we will model monthly BLS jobs data 12 months out in three different ways.  

## Part One: VAR Model (Connor)
Load Pagkages
```{r results = FALSE, messages= FALSE, results='hide', echo=FALSE, warning = FALSE}
library(tseries)
library(forecast)
library(vars)
library(tidyverse)
library(lubridate)
library(dynlm)
```

Load and clean data
```{r results = FALSE, messages= FALSE, results='hide', echo=FALSE, warning = FALSE}
# Load data from our Github repository
jobs_data <- read_csv("https://raw.githubusercontent.com/co237/forecasting_final/main/jobs_data.csv")
# Format date variable
jobs_data$Month <- as.Date(jobs_data$Month, "%m/%d/%Y")
# Drop pre-June 1976 data (and April 2021) for full panel
jobs1 = jobs_data[-c(1:449),]
jobs = jobs1[c(1:538),]
jobs <- jobs[ c(1,2,3,4,5,6,8,9,10,11,12,13,14) ]
# Create Time Series
jobs_national <- ts(jobs$Jobs,
                    start = c(1976,6),
                    end = c(2021, 3),
                    frequency = 12)

tenyr <- ts(jobs$Ten_yr_Treasury,
             start = c(1976,6),
             end = c(2021, 3),
             frequency = 12)

recession <- ts(jobs$NBER_Recession,
                      start = c(1976,6),
                      end = c(2021, 3),
                      frequency = 12)

population <- ts(jobs$Population,
                start = c(1976,6),
                end = c(2021, 3),
                frequency = 12)

Ind_Production <- ts(jobs$Industrial_Production,
                 start = c(1976,6),
                 end = c(2021, 3),
                 frequency = 12)

financial_conditions <- ts(jobs$Chi_Fed_NFCI,
                     start = c(1976,6),
                     end = c(2021, 3),
                     frequency = 12)

cpi <- ts(jobs$CPI,
                           start = c(1976,6),
                           end = c(2021, 3),
                           frequency = 12)

treasury_spread <- ts(jobs$T10Y2YM,
          start = c(1976,6),
          end = c(2021, 3),
          frequency = 12)

jobs_national_chg <- diff(jobs_national, lag = 1)
jobs_national_chg <- ts(jobs_national_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
tenyr_chg <- diff(tenyr, lag = 1)
tenyr_chg <- ts(tenyr_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
population_chg<- diff(population, lag = 1)
population_chg <- ts(population_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
Ind_Production_chg<- diff(Ind_Production, lag = 1)
Ind_Production_chg <- ts(Ind_Production_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
financial_conditions_chg<- diff(financial_conditions, lag = 1)
financial_conditions_chg <- ts(financial_conditions_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
cpi_chg<- diff(cpi, lag = 1)
cpi_chg <- ts(cpi_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)
treasury_spread_chg<- diff(treasury_spread, lag = 1)
treasury_spread_chg <- ts(treasury_spread_chg,
          start = c(1976,7),
          end = c(2021, 3),
          frequency = 12)

```

Total U.S. (non-farm) employment over time:

```{r}
plot(jobs_national, ylab = "Jobs", main = "U.S. Non-Farm Employment, 1976-Today")
plot(jobs_national_chg, ylab = "Jobs", main = "Monthly Job Gain/Joss, 1976-Today")
```

Model: VAR with one and two lag variables
```{r results = FALSE, messages= FALSE, results='hide', echo=FALSE, warning = FALSE}
VAR_data <- window(ts.union(jobs_national_chg, tenyr_chg, recession, population_chg, Ind_Production_chg, financial_conditions_chg, cpi_chg, treasury_spread_chg), start = c(1976, 7), end = c(2021, 3))
VAR_est <- VAR(y =VAR_data, p = 2)
VAR_est     

```

Forecast 12 months out
```{r}
forecast_data <- forecast(VAR_est, h = 12)

forecasted_jobs_chg <- ts(forecast_data$forecast$jobs_national_chg$mean,
                      start = c(2021,4),
                           end = c(2022, 3),
                           frequency = 12)

jobs_since_2010_chg <- ts(jobs_national_chg[404:538],
                  start = c(2010,1),
                           end = c(2021, 3),
                           frequency = 12)
seqplot.ts(jobs_since_2010_chg, forecasted_jobs_chg, ylab = "Monthly chang ein jobs", main = "Actual (Black) and Forecasted (Red) U.S. Non-Farm Job Growth")


```

Look-back
```{r}
VAR_data_lookback <- window(ts.union(jobs_national_chg, tenyr_chg, recession, population_chg, Ind_Production_chg, financial_conditions_chg, cpi_chg, treasury_spread_chg), start = c(1976, 7), end = c(2018, 12))

VAR_est2 <- VAR(y =VAR_data_lookback, p = 2)

forecast_data_2 <- forecast(VAR_est2, h = 12)


forecasted_jobs2_chg <- ts(forecast_data_2$forecast$jobs_national_chg$mean,
                      start = c(2019,1),
                           end = c(2019, 12),
                           frequency = 12)

recent_jobs_chg <- ts(jobs_national_chg[464:523],
                  start = c(2015,1),
                           end = c(2019, 12),
                           frequency = 12)

ts.plot(forecast_data_2$forecast$jobs_national_chg$mean, recent_jobs_chg,gpars = list(col = c("red", "black")), Main="Monthly Job Growth vs Predicted", ylab = "Monthly Jobs Added")

accuracy(forecasted_jobs2_chg, recent_jobs_chg)


```


##  Part Two: STL Decomposition (Ross )


```{r set-options, echo=FALSE, include = FALSE}
options(width=120)
#options("scipen"=100, "digits"=8)
options("warn" = -1)
library(dplyr)
library(xts)
library(forecast)
library(lubridate)
library(ggplot2)
library(quantmod)
library(TSstudio)
library(astsa)
library(urca)
library(tseries)
```

### {.tabset .tabset-pills}

### Load data

```{r, echo = FALSE}
# Pull total nonfarm employment from FRED
getSymbols('PAYEMS', src = 'FRED') # All Employees, Total Nonfarm, Establishment Survey
# Split into test and train samples
df <- xts_to_ts(window(PAYEMS), frequency = 12)
df.train <- xts_to_ts(window(PAYEMS, end = '2014-12-31'), frequency = 12)
df.test <- xts_to_ts(window(PAYEMS, start = '2015-01-01', end = '2021-05-01'), frequency = 12)
df.test.pre.covid <- xts_to_ts(window(PAYEMS, start = '2015-01-01', end = '2019-12-31'))
# Plot the time series
autoplot(PAYEMS, main = 'Total Nonfarm Payrolls') + ylab('')
```

### Exploratory data analysis

Here we look at plots of the training and test samples of the raw data. Looking
at the ADF test we can see that the time series is **not** stationary, with a p-value
of X. However, when the data are first differenced the p-value is Y, and we can reject the null hypothesis that the data aren't stationary.


```{r}
# Plot the data
autoplot(window(df.train, start = 1940), main = 'Level') +
    geom_line(linetype = "dashed", color = 'black') + 
    geom_point(color = 'blue', size = 2) +
    autolayer(df.test, series = 'Test') + 
    theme(aspect.ratio = .75) + ylab('')
autoplot(window(diff(df.train), start = 1940), main = 'First differences') +
    geom_line(linetype = "dashed", color = 'black') + 
    geom_point(color = 'blue', size = 2) +
    autolayer(diff(df.test), series = 'Test') + 
    theme(aspect.ratio = .75) + ylab('')
acf(df, main = 'ACF of training data')
# Check for stationarity using the augmented dickey fuller test & KPSS Unit Root Test
df %>% tseries::adf.test() %>% print()
df %>% ur.kpss() %>% summary()
# Now check the differenced series
df %>% diff() %>% tseries::adf.test() %>% print()
df %>% diff() %>% ur.kpss() %>% summary()
```

### STL decomposition

Here we use the seasonal and trend decomposition using Loess. This model is particularly
well suited for this time-series since the seasonal component can change over time - this series
is cyclical by nature, but the cycles don't follow a specific period.

There are two parameters we need to set. 

1. `t.window` - controls how rapidly the trend-cycle component can change
2. `s.window` - controls how rapidly the seasonal component can change

They specify the number of consecutive years to use when estimating the trend-cycle and seasonal components

```{r, echo = FALSE}
# Run the STL model on levels
stl_model <- stl(df.train[,1], s.window = "periodic", t.window = 13, robust = TRUE)
summary(stl_model)
# Create forecasts for the pre-covid and post covid periods
stl_fcast_pre_covid <- forecast(stl_model, h = 60)
stl_fcast_full <- forecast(stl_model, h = 76)
# Test accuracy
print('Pre-covid')
accuracy(stl_fcast_pre_covid, df.test.pre.covid) %>% print()
print('All data')
accuracy(stl_fcast_full, df.test) %>% print()
# Look at the decomposition
df.train[,1] %>% stl(s.window = "periodic", t.window = 13, robust = TRUE) %>% autoplot()
autoplot(stl_fcast_pre_covid, main = '') +
    autolayer(df.test.pre.covid)
autoplot(stl_fcast_full, main = '') + 
    autolayer(df.test)
# Run the STL model on differences
stl_model <- stl(diff(df.train[,1]), s.window = "periodic", t.window = 13, robust = TRUE)
summary(stl_model)
# Create forecasts for the pre-covid and post covid periods
stl_fcast_pre_covid <- forecast(stl_model, h = 60)
stl_fcast_full <- forecast(stl_model, h = 76)
# Test accuracy
print('Pre-covid')
accuracy(stl_fcast_pre_covid, diff(df.test.pre.covid)) %>% print()
print("All data")
accuracy(stl_fcast_full, diff(df.test)) %>% print()
# Look at the decomposition
df.train[,1] %>% diff() %>% stl(s.window = "periodic", t.window = 13, robust = TRUE) %>% autoplot()
autoplot(stl_fcast_pre_covid, main = '') +
    autolayer(diff(df.test.pre.covid), series = 'Actual') 
autoplot(stl_fcast_full, main = '') +
    autolayer(diff(df.test), series = 'Actual') 
```


## Part Three (Zakir)
---
title: "LSTM Model"
author: "ZAKIR BEKENOV"
---


#Packages
install.packages("keras")
install.packages("tensorflow")
install.packages("Metrics")

```{r, eval =FALSE}
#Library
library(keras)
library(tensorflow)
library(ggplot2)
library(stats)
library(readr)
library(dplyr)
library(forecast)
library(Metrics)
```

```{r, eval =FALSE}
getwd()
setwd("/Users/zakirbekenov/Desktop/Spring 2021/Time Series Analysis/project")
df <- read_csv("PAYEMS.csv")
head(df)

structure(df)
df$DATE <- as.Date(df$DATE, "%Y-%M-%D")
ggplot(df, aes(x=DATE, y = PAYEMS)) + geom_line()
```

```{r, eval =FALSE}
#Transform data to stationary
##Remove first row as we cannot difference
dateID <- df$DATE[2:nrow(df)]
diff <- diff(df$PAYEMS, differencs = 1)
head(diff)
```

```{r, eval =FALSE}
##LSTM expects the data to be in a supervised learning mode

supervised <- as.data.frame(cbind(lag(diff,1), diff))
supervised[is.na(supervised)] <- 0
head(supervised)
```

```{r, eval =FALSE}
#Split dataset into training and testing sets
#Unlike in most analysis where training and testing data sets are randomly sampled, 
##with time series data the order of the observations does matter.
n_ <- round(nrow(df) * .7, digits = 0)
train <- supervised[1:n_,]
test <- supervised[(n_+1):N,]
train_id <- dateID[1:n_]
test_id <- dateID[(n_+1)]

str(test)
plot(test)

#Normalize the data
## scale data
scale_data <- function(train, test, feature_range = c(0,1)) {
  x = train
  fr_min = feature_range[1]
  fr_max = feature_range[2]
  std_train = (x - min(x)) / (max(x) - min(x))
  std_test = (test - min(x)) / (max(x) - min(x))
  
  scaled_train = std_train * (fr_max - fr_min) + fr_min
  scaled_test = std_test * (fr_max - fr_min) + fr_min
  
  return( list(scaled_train = as.vector(scaled_train), scaled_test = as.vector(scaled_test) ,scaler= c(min =min(x), max = max(x))) )
}

#Function to reverse scale data for prediction
reverse_scaling <- function(scaled, scaler, feature_range = c(0,1)) {
  min = scaler[1]
  max = scaler[2]
  t = length(scaled)
  mins = feature_range[1]
  maxs = feature_range[2]
  inverted_dfs = numeric(t)
  
  for(i in 1:t) {
    X = (scaled[i] - mins) / (maxs - mins)
    rawValues = X * (max - min) + min
    inverted_dfs[i] <- rawValues
  }
  return(inverted_dfs)
}

Scaled <- scale_data(train, test, c(-1,1))

x_train <- Scaled$scaled_train[,1]
y_train <- Scaled$scaled_train[,2]

x_test <- Scaled$scaled_test[,1]
y_test <- Scaled$scaled_test[,2]

#Modeling
##Define the model
# Reshape the input to 3-dim
dim(x_train) <- c(length(x_train), 1,1)

# specify required arguments

X_shape2 <- dim(x_train)[2]
X_shape3 <- dim(x_train)[3]
batch_size <- 1
units <- 100
n_timesteps <- 12
n_predictions <- n_timesteps

build_matrix <- function(tseries, overall_timesteps) {
  t(sapply(1:(length(tseries) - overall_timesteps + 1), function(x) 
    tseries[x:(x + overall_timesteps - 1)]))
}
reshape_X_3d <- function(X) {
  dim(X) <- c(dim(X)[1], dim(X)[2], 1)
  X
}

model <- keras_model_sequential()
model %>% 
  layer_lstm(units, batch_input_shape = c(batch_size, X_shape2, X_shape3), stateful = TRUE) %>%
  layer_dense(units = 10)

#Compile the model
#Adaptive Monument Estimation (ADAM)
model %>% 
  compile(loss = 'mean_squared_error',
          optimizer = optimizer_adam(lr = 0.03, decay = 1e-6),
          metrics = c('accuracy')
  )
model
#Fit the model
##We set the argument shuffle = FALSE to avoid shuffling the training set
Epochs = 5   
for(i in 1:Epochs ){
  model %>% fit(x_train, y_train, epochs=1, batch_size=batch_size, verbose=1, shuffle=FALSE)
  model %>% reset_states()
}

#Make predictions
L = length(x_test)
scaler = Scaled$scaler
predictions = numeric(L)

for(i in 1:L){
  X = x_test[i]
  dim(X) = c(1,1,1)
  yhat = model %>% predict(X, batch_size=batch_size)
  # invert scaling
  yhat = invert_scaling(yhat, scaler,  c(-1, 1))
  # invert differencing
  yhat  = yhat + df$PAYEMS[(n+i)]
  # store
  predictions[i] <- yhat
}
head(predictions)
lstmMAE <- mae(df$PAYEMS, predictions)
head(lstmMAE)
```
## Including Plots

You can also embed plots, for example:

```{r, eval =FALSE}
plot.ts(predictions, col = "red")
```
